{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import openai\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API key\n",
    "openai.organization =  os.environ[\"OPENAI_ORGANIZATION\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "voice_id =  os.environ[\"VOICE_ID\"]\n",
    "# ElevenLabs API configuration\n",
    "CHUNK_SIZE = 1024\n",
    "url = \"https://api.elevenlabs.io/v1/text-to-speech/{}\".format(voice_id)\n",
    "headers = {\n",
    "    \"xi-api-key\": os.environ['ELVEVEN_API_KEY']\n",
    "}\n",
    "\n",
    "# DID authentication data\n",
    "username = os.environ[\"DID_USERNAME\"]\n",
    "password = os.environ[\"DID_PASSWORD\"]\n",
    "base64_credentials = base64.b64encode(f\"{username}:{password}\".encode()).decode()\n",
    "headers_base = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Basic {base64_credentials}\"\n",
    "}\n",
    "\n",
    "# Template for the assistant's behavior\n",
    "template = \"\"\"Actua como un experto en IA, siempre defindes uso de la IA y vez el lado positivo de la AI y su uso para solucionar multiples problemas, ademas respondes de una manera muy breve y concisa y con una perspectiva muy positiva y esperanzadora\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model = 'gpt-3.5-turbo',temperature=.8,max_tokens=300),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transcribe_audio_to_text(recording):\n",
    "    try:\n",
    "        with open(recording, \"rb\") as audio_file:\n",
    "            transcribe = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        return transcribe['text']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error transcribing audio: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def generate_llm_response(transcribed_text, messages):\n",
    "    try:\n",
    "        output = chatgpt_chain.predict(human_input=transcribed_text)\n",
    "        messages.extend(['Alarcón: ' + transcribed_text, 'Alarcón pro IA: ' + output])\n",
    "        chat_transcription = \"\\n \".join(messages)\n",
    "        return chat_transcription, output\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating LLM response: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "def convert_text_to_audio(text):\n",
    "    try:\n",
    "        data = {\n",
    "            \"text\": text,\n",
    "            \"model_id\": \"eleven_multilingual_v1\",\n",
    "            \"voice_settings\": {\n",
    "                \"stability\": 0.45,\n",
    "                \"similarity_boost\": 0.85,\n",
    "                \"use_speaker_boost\": True\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        audio_path = 'output_.mp3'\n",
    "        with open(audio_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return audio_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting text to audio: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_did_audio(filename):\n",
    "    try:\n",
    "        url_audio = \"https://api.d-id.com/audios\"\n",
    "        files = { \"audio\": (filename, open(filename, \"rb\"), \".\") }\n",
    "        response_upload = requests.post(url_audio, files=files, headers=headers_base)\n",
    "        response_upload.raise_for_status()\n",
    "        audio_url = response_upload.json().get('url', '')\n",
    "        return audio_url\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading audio to D-ID: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def did_avatar(audio_url):\n",
    "    try:\n",
    "        url_talks = \"https://api.d-id.com/talks\"\n",
    "        data = {\n",
    "            \"source_url\": \"https://create-images-results.d-id.com/google-oauth2%7C110735661733404113770/upl_rXLLDq6unvtWbkA5q3Cc1/image.png\",\n",
    "            \"script\": {\n",
    "                \"type\": \"audio\",\n",
    "                \"audio_url\": audio_url\n",
    "            }\n",
    "        }\n",
    "        headers_talk = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Basic {base64_credentials}\"\n",
    "        }\n",
    "        response_talk = requests.post(url_talks, json=data, headers=headers_talk)\n",
    "        response_talk.raise_for_status()\n",
    "        request_video_id = response_talk.json().get('id', '')\n",
    "        url_video_talks = f\"https://api.d-id.com/talks/{request_video_id}\"\n",
    "\n",
    "        flag_validation = True\n",
    "        while flag_validation:\n",
    "            response_video = requests.get(url_video_talks, headers=headers_base)\n",
    "            response_video.raise_for_status()\n",
    "            try:\n",
    "                video_url = response_video.json()['result_url']\n",
    "                flag_validation = False\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "        return video_url\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating avatar with D-ID: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def download_video(video_url):\n",
    "    file_name = f'video_{uuid.uuid4()}.mp4'\n",
    "    try:\n",
    "        response = requests.get(video_url)\n",
    "        response.raise_for_status()\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return file_name\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading video: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#avatar_talking(\"flagged/audio/5d0656b0bac11c148529d7c75a50c60b3fe6f2f1/tmpu8inxq6k.wav\", \"pro.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "def avatar_talking(audio, img):\n",
    "    global messages\n",
    "    try:\n",
    "        text_transcribe = transcribe_audio_to_text(audio)\n",
    "        llm_response, last_response = generate_llm_response(text_transcribe, messages)\n",
    "        audio_path = convert_text_to_audio(last_response)\n",
    "        if not audio_path:\n",
    "            raise Exception(\"Error converting text to audio.\")\n",
    "        \n",
    "        audio_url = upload_did_audio(audio_path)\n",
    "        video_url = did_avatar(audio_url)\n",
    "        if not video_url:\n",
    "            raise Exception(\"Error generating video avatar.\")\n",
    "        \n",
    "        video_path = download_video(video_url)\n",
    "        if not video_path:\n",
    "            raise Exception(\"Error downloading video.\")\n",
    "        \n",
    "        return video_path, llm_response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in avatar_talking: {e}\")\n",
    "        return \"\", \"An error occurred. Please try again.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "default_image_path = \"pro.png\"  # Path to the default image\n",
    "\n",
    "ui = gr.Interface(\n",
    "    fn=avatar_talking,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\"),\n",
    "        gr.Image(label=\"Image\", value=default_image_path, height=250, width=250)  # Default image set here\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Video(label=\"Generated Audio\", autoplay=True),\n",
    "        gr.Textbox(label=\"Transcript\")\n",
    "    ],\n",
    "    live=False  # Optional: Enables live updates as you speak, set to False for this use case\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ui.launch(debug=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
